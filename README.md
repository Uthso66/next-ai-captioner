# ğŸ§  AI Image Captioner â€” Next.js + FastAPI + BLIP

![Next.js](https://img.shields.io/badge/Next.js-000000?style=for-the-badge\&logo=nextdotjs\&logoColor=white)
![TypeScript](https://img.shields.io/badge/TypeScript-3178C6?style=for-the-badge\&logo=typescript\&logoColor=white)
![FastAPI](https://img.shields.io/badge/FastAPI-009688?style=for-the-badge\&logo=fastapi\&logoColor=white)
![Python](https://img.shields.io/badge/Python-3776AB?style=for-the-badge\&logo=python\&logoColor=white)
![Tailwind CSS](https://img.shields.io/badge/Tailwind_CSS-38B2AC?style=for-the-badge\&logo=tailwindcss\&logoColor=white)
![PNPM](https://img.shields.io/badge/pnpm-F69220?style=for-the-badge\&logo=pnpm\&logoColor=white)

A full-stack **AI Image Captioning** web application that generates natural-language descriptions for uploaded images using a **locally hosted BLIP model**.
The frontend is built with **Next.js + TypeScript**, while the backend runs a **FastAPI + PyTorch** inference server.

ğŸ‘‰ **Live Demo (Frontend):**
[https://next-ai-captioner-wbpp-p8nqpxocr-uthsos-projects.vercel.app/](https://next-ai-captioner-wbpp-p8nqpxocr-uthsos-projects.vercel.app/)

---

## ğŸš€ Features

* ğŸ§  AI-powered image caption generation (BLIP)
* ğŸ“¸ Image upload with real-time preview
* ğŸ§¾ Clean, readable natural-language captions
* âš¡ FastAPI backend with model caching
* ğŸŒ Fully decoupled frontend & backend architecture
* ğŸ¨ Minimal, responsive UI built with Tailwind CSS
* ğŸ§± Modern Next.js App Router setup

---

## ğŸ§± Tech Stack

| Layer               | Technology           |
| :------------------ | :------------------- |
| **Frontend**        | Next.js (App Router) |
| **Language**        | TypeScript           |
| **Styling**         | Tailwind CSS         |
| **Backend**         | FastAPI              |
| **ML Model**        | BLIP (Salesforce)    |
| **ML Framework**    | PyTorch              |
| **Package Manager** | PNPM                 |
| **Deployment (FE)** | Vercel               |
| **Deployment (BE)** | Railway              |

---

## ğŸ¥ Demo Video

> ğŸ“Œ **Video walkthrough & live inference demo**

ğŸ‘‰ *(https://youtu.be/uCn8YmCGg0A)*
---

## ğŸ§  How It Works

1. User uploads an image from the browser
2. Image is sent to the Next.js API route
3. The API forwards the image to the FastAPI backend
4. FastAPI runs the BLIP model locally (CPU)
5. Generated caption is returned and displayed in the UI

This approach avoids third-party inference instability and keeps full control over the ML pipeline.

---

## ğŸ’» Local Setup

### 1ï¸âƒ£ Clone the repository

```bash
git clone https://github.com/Uthso66/next-ai-captioner.git
cd next-ai-captioner
```

---

### 2ï¸âƒ£ Frontend (Next.js)

```bash
pnpm install
pnpm run dev
```

Frontend will be available at:
ğŸ‘‰ [http://localhost:3000](http://localhost:3000)

---

### 3ï¸âƒ£ Backend (FastAPI)

```bash
cd backend
pip install -r requirements.txt
uvicorn main:app --host 0.0.0.0 --port 8000
```

Backend health check:
ğŸ‘‰ [http://localhost:8000/health](http://localhost:8000/health)

---

## ğŸ§ª Notes & Limitations

* BLIP runs on **CPU only** (free deployment)
* First request may be slow due to model warm-up
* Designed for **demo / portfolio / learning purposes**
* Not optimized for high-traffic production workloads

---

## ğŸ’¡ Motivation

This project was built to:

* Avoid unreliable third-party inference APIs
* Demonstrate **full-stack AI deployment**
* Gain hands-on experience with **model serving**
* Showcase real-world system design beyond frontend-only demos

---

## ğŸ§” Author

**Uthso**
*Software QA Engineer â€¢ Security Enthusiast â€¢ AI/ML Hobbyist*

ğŸŒ [Portfolio](https://www.google.com/search?q=%23)
ğŸ™ [GitHub](https://github.com/Uthso66)
ğŸ§  [LinkedIn](https://www.linkedin.com/in/tarikul-islam-uthso/)

---

## ğŸª„ License

This project is licensed under the **MIT License** Â© 2025 Uthso

